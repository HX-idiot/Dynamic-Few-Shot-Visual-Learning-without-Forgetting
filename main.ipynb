{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchnet as tnt\n",
    "\n",
    "#modify the data root\n",
    "_MINI_IMAGENET_DATASET_DIR = '../datasets/MiniImagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file,'rb') as f:\n",
    "        data=pickle.load(f,encoding='iso-8859-1')\n",
    "    return data\n",
    "\n",
    "def buildLabelIndex(labels):\n",
    "    label2inds={}\n",
    "    for idx,label in enumerate(labels):\n",
    "        if label not in label2inds:\n",
    "            label2inds[label]=[]\n",
    "        label2inds[label].append(idx)\n",
    "    return label2inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniImageNet(data.Dataset):\n",
    "    def __init__(self,phase='train',do_not_use_random_transf=False):\n",
    "        self.base_folder='miniImagenet'\n",
    "        assert(phase=='train' or phase=='val' or phase=='test')\n",
    "        self.phase=phase\n",
    "        self.name='MiniImageNet_'+phase\n",
    "        \n",
    "        print('Loading mini ImageNet dataser - phase {0}'.format(phase))\n",
    "        file_train_categories_train_phase = os.path.join(_MINI_IMAGENET_DATASET_DIR,\n",
    "                                                        'miniImageNet_category_split_train_phase_train.pickle')\n",
    "        file_train_categories_val_phase = os.path.join(_MINI_IMAGENET_DATASET_DIR,\n",
    "                                                      'miniImageNet_category_split_train_phase_val.pickle')\n",
    "        file_train_categories_test_phase = os.path.join(_MINI_IMAGENET_DATASET_DIR,\n",
    "                                                       'miniImageNet_category_split_train_phase_test.pickle')\n",
    "        file_val_categories_val_phase = os.path.join(_MINI_IMAGENET_DATASET_DIR,\n",
    "                                                    'miniImageNet_category_split_val.pickle')\n",
    "        file_test_categories_test_phase = os.path.join(_MINI_IMAGENET_DATASET_DIR,\n",
    "                                                      'miniImageNet_category_split_test.pickle')\n",
    "        \n",
    "        if self.phase=='train':\n",
    "            #during training phase we only load the training phase images of the training category\n",
    "            data_train=load_data(file_train_categories_train_phase)\n",
    "            self.data=data_train['data'] #array (n,84,84,3)\n",
    "            self.labels=data_train['labels'] #list[n]\n",
    "            \n",
    "            self.label2ind=buildLabelIndex(self.labels)\n",
    "            self.labelIds=sorted(self.label2ind.keys())\n",
    "            self.num_cats=len(self.labelIds)\n",
    "            self.labelIds_base=self.labelIds\n",
    "            self.num_cats_base=len(self.labelIds_base)\n",
    "        elif self.phase=='val' or self.phase=='test':\n",
    "            if self.phase=='test':\n",
    "                data_base=load_data(file_train_categories_test_phase)\n",
    "                data_novel=load_data(file_test_categories_test_phase)\n",
    "            else:\n",
    "                data_base=load_data(file_train_categories_val_phase)\n",
    "                data_novel=load_data(file_val_categories_val_phase)\n",
    "            \n",
    "            self.data=np.concatenate(\n",
    "                [data_base['data'],data_novel['data']],axis=0)\n",
    "            self.labels=data_base['labels']+data_novel['labels']\n",
    "            \n",
    "            self.label2ind=buildLabelIndex(self.labels)\n",
    "            self.labelIds=sorted(self.label2ind.keys())\n",
    "            self.num_cats=len(self.labelIds)\n",
    "            \n",
    "            self.labelIds_base=buildLabelIndex(data_base['labels']).keys()\n",
    "            self.labelIds_novel=buildLabelIndex(data_novel['labels']).keys()\n",
    "            self.num_cats_base=len(self.labelIds_base)\n",
    "            self.num_cats_novel=len(self.labelIds_novel)\n",
    "            intersection=set(self.labelIds_base) & set(self.labelIds_novel)\n",
    "            assert(len(intersection)==0)\n",
    "        else:\n",
    "            raise ValueError('Not valid phase {0}'.fotmat(self.phase))\n",
    "        \n",
    "        mean_pix = [x/255.0 for x in [120.39586422,  115.59361427, 104.54012653]]\n",
    "        std_pix = [x/255.0 for x in [70.68188272,  68.27635443,  72.54505529]]\n",
    "        normalize = transforms.Normalize(mean=mean_pix, std=std_pix)\n",
    "        \n",
    "        if (self.phase=='test' or self.phase=='val') or (do_not_use_random_transf==True):\n",
    "            self.transform=transforms.Compose([\n",
    "                lambda x:np.array(x),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.transform=transforms.Compose([\n",
    "                transforms.RandomCrop(84,padding=8),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                lambda x : np.array(x),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        img,label=self.data[index],self.labels[index]\n",
    "        #doing this so that it is consistent with all other datasets to return a PIL image\n",
    "        img=Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            img=self.transform(img)\n",
    "        return img,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataloader():\n",
    "    def __init__(self,dataset,\n",
    "                nKnovel=5,#number of novel categories\n",
    "                nKbase=-1,#number of base categories\n",
    "                nExemplars=1,#number of training examples per novel category\n",
    "                nTestNovel=15*5,#number of test examples for all novel categories\n",
    "                nTestBase=15*5,#number of test examples for all base categories\n",
    "                batch_size=1,#number of training episodes per batch\n",
    "                num_workers=4,\n",
    "                epoch_size=2000):\n",
    "        self.dataset=dataset\n",
    "        self.phase=self.dataset.phase\n",
    "        max_possible_nKnovel=(self.dataset.num_cats_base if self.phase=='train'\n",
    "                             else self.dataset.num_cats_novel)\n",
    "        assert(nKnovel>=0 and nKnovel<max_possible_nKnovel)\n",
    "        self.nKnovel=nKnovel\n",
    "        \n",
    "        max_possible_nKbase=self.dataset.num_cats_base\n",
    "        nKbase=nKbase if nKbase>=0 else max_possible_nKbase\n",
    "        \n",
    "        if self.phase=='train' and nKbase>0:\n",
    "            nKbase-=self.nKnovel\n",
    "            max_possible_nKbase-=self.nKnovel\n",
    "            \n",
    "        assert(nKbase>=0 and nKbase <=max_possible_nKbase)\n",
    "        self.nKbase=nKbase\n",
    "        \n",
    "        self.nExemplars=nExemplars\n",
    "        self.nTestNovel=nTestNovel\n",
    "        self.nTestBase=nTestBase\n",
    "        self.batch_size=batch_size\n",
    "        self.epoch_size=epoch_size\n",
    "        self.num_workers=num_workers\n",
    "        self.is_eval_mode=(self.phase=='test') or (self.phase=='val')\n",
    "        \n",
    "    def sampleImageIdsFrom(self, cat_id , sample_size=1):\n",
    "        \"\"\"\n",
    "        samples 'sample_size' number of unique image ids picked from the\n",
    "        category 'cat_id'\n",
    "        \"\"\"\n",
    "        assert(cat_id in self.dataset.label2ind)\n",
    "        assert(len(self.dataset.label2ind[cat_id])>=sample_size)\n",
    "        #Note : random.sample samples elements without replacement.\n",
    "        return random.sample(self.dataset.label2ind[cat_id],sample_size)\n",
    "    \n",
    "    def sampleCategories(self,cat_set,sample_size=1):\n",
    "        \"\"\"\n",
    "        Samples 'sample_size' number of unique categories picked from the\n",
    "        'cat_set' set of categories.'cat_set' can be either 'base' or 'novel'.\n",
    "        \"\"\"\n",
    "        if cat_set=='base':\n",
    "            labelIds=self.dataset.labelIds_base\n",
    "        elif cat_set=='novel':\n",
    "            labelIds=self.dataset.labelIds_novel\n",
    "        else:\n",
    "            raise ValueError('Not recognize category set {}'.format(cat_set))\n",
    "            \n",
    "        assert(len(labelIds)>=sample_size)\n",
    "        \n",
    "        return random.sample(labelIds,sample_size)\n",
    "    \n",
    "    def sample_base_and_novel_categories(self,nKbase , nKnovel):\n",
    "        \"\"\"\n",
    "        Samples 'nKbase' number of base categories and 'nKnovel'  number of novel categories.\n",
    "        \"\"\"\n",
    "        if self.is_eval_mode:\n",
    "            assert(nKnovel<=self.dataset.num_cats_novel)\n",
    "            \n",
    "            Kbase = sorted(self.sampleCategories('base',nKbase))\n",
    "            Knovel = sorted(self.sampleCategories('novel',nKnovel))\n",
    "        else:\n",
    "            cats_ids = self.sampleCategories('base',nKbase+nKnovel)\n",
    "            assert(len(cats_ids)==(nKbase+nKnovel))\n",
    "            \n",
    "            random.shuffle(cats_ids)\n",
    "            Knovel=sorted(cats_ids[:nKnovel])\n",
    "            Kbase=sorted(cats_ids[nKnovel:])\n",
    "        return Kbase,Knovel\n",
    "    \n",
    "    def sample_test_examples_for_base_categories(self,Kbase,nTestBase):\n",
    "        \"\"\"\n",
    "        Sample 'nTestBase' number of images from the 'Kbase' categories.\n",
    "    \n",
    "        \"\"\"\n",
    "        Tbase=[]\n",
    "        if len(Kbase)>0:\n",
    "            KbaseIndices=np.random.choice(np.arange(len(Kbase)),size=nTestBase,replace=True)\n",
    "            KbaseIndices,NumImagesPerCategory=np.unique(KbaseIndices,return_counts=True)\n",
    "            \n",
    "            for Kbase_idx,NumImages in zip(KbaseIndices,NumImagesPerCategory):\n",
    "                imd_ids=self.sampleImageIdsFrom(Kbase[Kbase_idx],sample_size=NumImages)\n",
    "                Tbase+=[(img_id,Kbase_idx) for img_id in imd_ids]\n",
    "        assert(len(Tbase)==nTestBase)\n",
    "        \n",
    "        return Tbase\n",
    "    \n",
    "    def sample_train_and_test_examples_for_novel_categories(\n",
    "        self,Knovel,nTestNovel,nExemplars,nKbase):\n",
    "        \"\"\"\n",
    "        Samples train and test examples of the novel categories.\n",
    "        \n",
    "        Args:\n",
    "            Knovel:a list with the ids of the novel categories\n",
    "            nTestNovel:the total number of test imgs that will be sampled from all novel categories\n",
    "            nExemplars:the number of training examples per novel category that will be sampled\n",
    "            nKbase:the number of base categories.it's used as offset of the category index of each sampled img\n",
    "            \n",
    "        Returns:\n",
    "            Tnovel:a list of length 'nTestNovel' with 2-element tuple.\n",
    "                    (img_id , category_label)\n",
    "            Exemplars: a list of length len(Knovel)*nExemplars of 2-element tuple\n",
    "                    (img_id , category_label range in [nKbase,nKbase+len(Knovel)-1])\n",
    "        \"\"\"\n",
    "        if len(Knovel)==0:\n",
    "            return [],[]\n",
    "        \n",
    "        nKnovel=len(Knovel)\n",
    "        Tnovel=[]\n",
    "        Exemplars=[]\n",
    "        assert((nTestNovel % nKnovel)==0)\n",
    "        nEvalExamplesPerClass = int(nTestNovel/nKnovel)\n",
    "        \n",
    "        for Knovel_idx in range(len(Knovel)):\n",
    "            imd_ids=self.sampleImageIdsFrom(Knovel[Knovel_idx],sample_size=(nEvalExamplesPerClass+nExemplars))\n",
    "            imds_tnovel=imd_ids[:nEvalExamplesPerClass]\n",
    "            imds_ememplars=imd_ids[nEvalExamplesPerClass:]\n",
    "            \n",
    "            Tnovel+=[(img_id , nKbase+Knovel_idx) for img_id in imds_tnovel]\n",
    "            Exemplars+=[(img_id,nKbase+Knovel_idx) for img_id in imds_ememplars]\n",
    "        \n",
    "        assert(len(Tnovel)==nTestNovel)\n",
    "        assert(len(Exemplars)==len(Knovel)*nExemplars)\n",
    "        random.shuffle(Exemplars)\n",
    "        \n",
    "        return Tnovel,Exemplars\n",
    "    \n",
    "    def sample_episode(self):\n",
    "        \"\"\"\n",
    "        Sample a training episode\n",
    "        \"\"\"\n",
    "        nKnovel=self.nKnovel\n",
    "        nKbase=self.nKbase\n",
    "        nTestNovel=self.nTestNovel\n",
    "        nTestBase=self.nTestBase\n",
    "        nExemplars=self.nExemplars\n",
    "        \n",
    "        Kbase,Knovel = self.sample_base_and_novel_categories(nKbase,nKnovel)\n",
    "        Tbase=self.sample_test_examples_for_base_categories(Kbase,nTestBase)\n",
    "        Tnovel,Exemplars=self.sample_train_and_test_examples_for_novel_categories(Knovel,nTestNovel,nExemplars,nKbase)\n",
    "        \n",
    "        #concatenate the base and novel category examples\n",
    "        Test=Tbase+Tnovel\n",
    "        random.shuffle(Test)\n",
    "        Kall=Kbase+Knovel\n",
    "        \n",
    "        return Exemplars , Test , Kall , nKbase\n",
    "    \n",
    "    def createExamplesTensorData(self,examples):\n",
    "        \"\"\"\n",
    "        Create the examples image and label tensor data\n",
    "        \"\"\"\n",
    "        images=torch.stack(\n",
    "            [self.dataset[img_idx][0] for img_idx ,_ in examples],dim=0)\n",
    "        labels=torch.LongTensor([label for _,label in examples])\n",
    "        return images,labels\n",
    "    \n",
    "    def get_iterator(self,epoch=0):\n",
    "        rand_seed=epoch\n",
    "        random.seed(rand_seed)\n",
    "        np.random.seed(rand_seed)\n",
    "        \n",
    "        def load_function(iter_idx):\n",
    "            Exemplars,Test,Kall,nKbase = self.sample_episode()\n",
    "            Xt,Yt=self.createExamplesTensorData(Test)\n",
    "            Kall=torch.LongTensor(Kall)\n",
    "            if len(Exemplars)>0:\n",
    "                Xe,Ye=self.createExamplesTensorData(Exemplars)\n",
    "                return Xe,Ye,Xt,Yt,Kall,nKbase\n",
    "            else:\n",
    "                return Xt,Yt,Kall,nKbase\n",
    "            \n",
    "        tnt_dataset=tnt.dataset.ListDataset(\n",
    "            elem_list=range(self.epoch_size),load=load_function)\n",
    "        data_loader=tnt_dataset.parallel(\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=(0 if self.is_eval_mode else self.num_workers),\n",
    "            shuffle=(False if self.is_eval_mode else True))\n",
    "        \n",
    "        return data_loader\n",
    "    def __call__(self,epoch=0):\n",
    "        return self.get_iterator(epoch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.epoch_size/self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModule,self).__init__()\n",
    "        self.model_name=str(type(self))\n",
    "        \n",
    "    def load(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def save(self,path=None):\n",
    "        if path is None:\n",
    "            raise ValueError('Please specify the saving road!!!')\n",
    "        torch.save(self.state_dict(),path)\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels,out_channels,use_relu=True):\n",
    "    if use_relu:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "    \n",
    "class AvgBlock(BasicModule):\n",
    "    def __init__(self,nFeat):\n",
    "        super(AvgBlock,self).__init__()\n",
    "        \n",
    "    def forward(self,features_train , labels_train):\n",
    "        labels_train_transposed=labels_train.transpose(1,2)\n",
    "        weight_novel=torch.bmm(labels_train_transposed,features_train)\n",
    "        weight_novel=weight_novel.div(\n",
    "            labels_train_transposed.sum(dim=2,keepdim=True).expand_as(weight_novel))\n",
    "        return weight_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(ConvNet,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "            conv_block(3,64),\n",
    "            conv_block(64,64),\n",
    "            conv_block(64,128),\n",
    "            conv_block(128,128,use_relu=False),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out=self.encoder(x)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(BasicModule):\n",
    "    def __init__(self,nFeat,nKall,scale_att=10.0):\n",
    "        super(AttentionBlock,self).__init__()\n",
    "        \n",
    "        self.nFeat=nFeat\n",
    "        self.queryLayer=nn.Linear(nFeat,nFeat)\n",
    "        self.queryLayer.weight.data.copy_(\n",
    "            torch.eye(nFeat,nFeat)+torch.randn(nFeat,nFeat)*0.001)\n",
    "        self.queryLayer.bias.data.zero_()\n",
    "        \n",
    "        self.scale_att=nn.Parameter(torch.FloatTensor(1).fill_(scale_att),requires_grad=True)\n",
    "        wkeys=torch.FloatTensor(nKall,nFeat).normal_(0.0,np.sqrt(2.0/nFeat))\n",
    "        self.wkeys=nn.Parameter(wkeys,requires_grad=True)\n",
    "        \n",
    "    def forward(self,features_train,labels_train,weight_base,Kbase):\n",
    "        \n",
    "        batch_size,num_train_examples,num_features=features_train.size()\n",
    "        nKbase=weight_base.size(1) #[batch_size,nKbase,num_features]\n",
    "        labels_train_transposed=labels_train.transpose(1,2)\n",
    "        nKnovel=labels_train_transposed.size(1) #[batch_size,nKnovel,num_train_examples]\n",
    "        \n",
    "        features_train=features_train.view(batch_size*num_train_examples,num_features)\n",
    "        Qe=self.queryLayer(features_train)\n",
    "        Qe=Qe.view(batch_size,num_train_examples,self.nFeat)\n",
    "        Qe=F.normalize(Qe,p=2,dim=Qe.dim()-1,eps=1e-12)\n",
    "        \n",
    "        wkeys=self.wkeys[Kbase.view(-1)]\n",
    "        wkeys=F.normalize(wkeys,p=2,dim=wkeys.dim()-1,eps=1e-12)\n",
    "        #Transpose from[batch_size,nKbase,nFeat]->[batch_size,nFeat,nKbase]\n",
    "        wkeys=wkeys.view(batch_size,nKbase,self.nFeat).transpose(1,2)\n",
    "        \n",
    "        #Compute the attention coefficients\n",
    "        #AttenCoffiencients=Qe*wkeys -> \n",
    "        #[batch_size x num_train_examples x nKbase] =[batch_size x num_train_examples x nFeat] * [batch_size x nFeat x nKbase]\n",
    "        AttentionCoef=self.scale_att*torch.bmm(Qe,wkeys)\n",
    "        AttentionCoef=F.softmax(AttentionCoef.view(batch_size*num_train_examples,nKbase))\n",
    "        AttentionCoef=AttentionCoef.view(batch_size,num_train_examples,nKbase)\n",
    "        \n",
    "        #Compute the weight_novel\n",
    "        #weight_novel=AttentionCoef * weight_base ->\n",
    "        #[batch_size x num_train_examples x num_features] =[batch_size x num_train_examples x nKbase] * [batch_size x nKbase x num_features]\n",
    "        weight_novel=torch.bmm(AttentionCoef,weight_base)\n",
    "        #weight_novel=labels_train_transposed*weight_novel ->\n",
    "        #[batch_size x nKnovel x num_features] = [batch_size x nKnovel x num_train_examples] * [batch_size x num_train_examples x num_features]\n",
    "        weight_novel=torch.bmm(labels_train_transposed,weight_novel)\n",
    "        #div K-shot ,get avg\n",
    "        weight_novel=weight_novel.div(labels_train_transposed.sum(dim=2,keepdim=True).expand_as(weight_novel))\n",
    "        return weight_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDiag(BasicModule):\n",
    "    def __init__(self,num_features,bias=False):\n",
    "        super(LinearDiag,self).__init__()\n",
    "        weight=torch.FloatTensor(num_features).fill_(1)#initialize to the identity transform\n",
    "        self.weight=nn.Parameter(weight,requires_grad=True)\n",
    "        \n",
    "        if bias:\n",
    "            bias=torch.FloatTensor(num_features).fill_(0)\n",
    "            self.bias=nn.Parameter(bias,requires_grad=True)\n",
    "            \n",
    "        else:\n",
    "            self.register_parameter('bias',None)\n",
    "            \n",
    "    def forward(self,X):\n",
    "        assert(X.dim()==2 and X.size(1)==self.weight.size(0))\n",
    "        out=X*self.weight.expand_as(X)\n",
    "        if self.bias is not None:\n",
    "            out=out+self.bias.expand_as(out)\n",
    "            \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(BasicModule):\n",
    "    def __init__(self,nKall=64,nFeat=128*5*5,weight_generator_type='none'):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.nKall=nKall\n",
    "        self.nFeat=nFeat\n",
    "        self.weight_generator_type=weight_generator_type\n",
    "        \n",
    "        weight_base=torch.FloatTensor(nKall,nFeat).normal_(\n",
    "        0.0,np.sqrt(2.0/nFeat))\n",
    "        self.weight_base=nn.Parameter(weight_base,requires_grad=True)\n",
    "        self.bias=nn.Parameter(torch.FloatTensor(1).fill_(0),requires_grad=True)\n",
    "        scale_cls=10.0\n",
    "        self.scale_cls=nn.Parameter(torch.FloatTensor(1).fill_(scale_cls),requires_grad=True)\n",
    "        \n",
    "        if self.weight_generator_type=='none':\n",
    "            #if type is none , then feature averaging is being used.\n",
    "            #However,in this case the generator doesn't involve any learnable params ,thus doesn't require training\n",
    "            self.favgblock=AvgBlock(nFeat)\n",
    "        elif self.weight_generator_type=='attention_based':\n",
    "            scale_att=10.0\n",
    "            self.favgblock=AvgBlock(nFeat)\n",
    "            self.attentionBlock=AttentionBlock(nFeat,nKall,scale_att=scale_att)\n",
    "            \n",
    "            self.wnLayerFavg=LinearDiag(nFeat)\n",
    "            self.wnLayerWatt=LinearDiag(nFeat)\n",
    "        else:\n",
    "            raise ValueError('weight_generator_type is not supported!')\n",
    "            \n",
    "    def get_classification_weights(\n",
    "        self,Kbase_ids,features_train=None,labels_train=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Get the classification weights of the base and novel categories.\n",
    "            Kbase_ids:[batch_size , nKbase],the indices of base categories that used\n",
    "            features_train:[batch_size,num_train_examples(way*shot),nFeat]\n",
    "            labels_train :[batch_size,num_train_examples,nKnovel(way)] one-hot of features_train\n",
    "        \n",
    "        return:\n",
    "            cls_weights:[batch_size,nK,nFeat] \n",
    "        \"\"\"\n",
    "        #get the classification weights for the base categories\n",
    "        batch_size,nKbase=Kbase_ids.size()\n",
    "        weight_base=self.weight_base[Kbase_ids.view(-1)]\n",
    "        weight_base=weight_base.view(batch_size,nKbase,-1)\n",
    "        \n",
    "        #if training data for novel categories are not provided,return only base_weight\n",
    "        if features_train is None or labels_train is None:\n",
    "            return weight_base\n",
    "        \n",
    "        #get classification weights for novel categories\n",
    "        _,num_train_examples , num_channels=features_train.size()\n",
    "        nKnovel=labels_train.size(2)\n",
    "        \n",
    "        #before do cosine similarity ,do L2 normalize\n",
    "        features_train=F.normalize(features_train,p=2,dim=features_train.dim()-1,eps=1e-12)\n",
    "        if self.weight_generator_type=='none':\n",
    "            weight_novel=self.favgblock(features_train,labels_train)\n",
    "            weight_novel=weight_novel.view(batch_size,nKnovel,num_channels)\n",
    "        elif self.weight_generator_type=='attention_based':\n",
    "            weight_novel_avg=self.favgblock(features_train,labels_train)\n",
    "            weight_novel_avg=self.wnLayerFavg(weight_novel_avg.view(batch_size*nKnovel,num_channels))\n",
    "            \n",
    "            #do L2 for weighr_base\n",
    "            weight_base_tmp=F.normalize(weight_base,p=2,dim=weight_base.dim()-1,eps=1e-12)\n",
    "            \n",
    "            weight_novel_att=self.attentionBlock(features_train,labels_train,weight_base_tmp,Kbase_ids)\n",
    "            weight_novel_att=self.wnLayerWatt(weight_novel_att.view(batch_size*nKnovel,num_channels))\n",
    "            \n",
    "            weight_novel=weight_novel_avg+weight_novel_att\n",
    "            weight_novel=weight_novel.view(batch_size,nKnovel,num_channels)\n",
    "        else:\n",
    "            raise ValueError('weight generator type is not supported!')\n",
    "            \n",
    "        #Concatenate the base and novel classification weights and return\n",
    "        weight_both=torch.cat([weight_base,weight_novel],dim=1)#[batch_size ,nKbase+nKnovel , num_channel]\n",
    "        \n",
    "        return weight_both\n",
    "    \n",
    "    def apply_classification_weights(self,features,cls_weights):\n",
    "        \"\"\"\n",
    "        Apply the classification weight vectors to the feature vectors\n",
    "        Args:\n",
    "            features:[batch_size,num_test_examples,num_channels]\n",
    "            cls_weights:[batch_size,nK,num_channels]\n",
    "        Return:\n",
    "            cls_scores:[batch_size,num_test_examples(query set),nK]\n",
    "        \"\"\"\n",
    "        #do L2 normalize\n",
    "        features=F.normalize(features,p=2,dim=features.dim()-1,eps=1e-12)\n",
    "        cls_weights=F.normalize(cls_weights,p=2,dim=cls_weights.dim()-1,eps=1e-12)\n",
    "        cls_scores=self.scale_cls*torch.baddbmm(1.0,\n",
    "                    self.bias.view(1,1,1),1.0,features,cls_weights.transpose(1,2))\n",
    "        return cls_scores\n",
    "    \n",
    "    def forward(self,features_test,Kbase_ids,features_train=None,labels_train=None):\n",
    "        \"\"\"\n",
    "        Recognize on the test examples both base and novel categories.\n",
    "        Args:\n",
    "            features_test:[batch_size,num_test_examples(query set),num_channels]\n",
    "            Kbase_ids:[batch_size,nKbase] , the indices of base categories that are being used.\n",
    "            features_train:[batch_size,num_train_examples,num_channels]\n",
    "            labels_train:[batch_size,num_train_examples,nKnovel]\n",
    "            \n",
    "        Return:\n",
    "            cls_score:[batch_size,num_test_examples,nKbase+nKnovel]\n",
    "    \n",
    "        \"\"\"\n",
    "        cls_weights=self.get_classification_weights(\n",
    "            Kbase_ids,features_train,labels_train)\n",
    "        cls_scores=self.apply_classification_weights(features_test,cls_weights)\n",
    "        return cls_scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "### training step1 : training FE and pretrain cosine-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 =========training Feature Extractor and pretrain cosine-based classifier\n",
    "use_cuda=torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)\n",
    "torch.manual_seed(1234)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=31\n",
    "lr=0.1\n",
    "momentum=0.9\n",
    "weight_decay=5e-4\n",
    "\n",
    "dataset_train=MiniImageNet(phase='train')\n",
    "# dataset_test=MiniImageNet(phase='val')\n",
    "\n",
    "dloader_train=FewShotDataloader(dataset=dataset_train,\n",
    "                               nKnovel=0,\n",
    "                               nKbase=64,\n",
    "                               nExemplars=0,\n",
    "                               nTestNovel=0,\n",
    "                               nTestBase=32,\n",
    "                               batch_size=8,\n",
    "                               num_workers=1,\n",
    "                               epoch_size=8*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir('results/trace_file'):\n",
    "    os.makedirs('results/trace_file')\n",
    "    os.makedirs('results/pretrain_model')\n",
    "    \n",
    "trace_file=os.path.join('results','trace_file','pre_train_trace.txt')\n",
    "if os.path.isfile(trace_file):\n",
    "    os.remove(trace_file)\n",
    "    \n",
    "#model\n",
    "fe_model=ConvNet()\n",
    "classifier=Classifier()\n",
    "if use_cuda:\n",
    "    fe_model.cuda()\n",
    "    classifier.cuda()\n",
    "\n",
    "#optimizer\n",
    "optimizer_fe=torch.optim.SGD(fe_model.parameters(),lr=lr,nesterov=True , momentum=momentum,weight_decay=weight_decay)\n",
    "optimizer_classifier=torch.optim.SGD(classifier.parameters(),lr=lr,nesterov=True , momentum=momentum,weight_decay=weight_decay)\n",
    "lr_schedule_fe=torch.optim.lr_scheduler.StepLR(optimizer=optimizer_fe,gamma=0.5,step_size=25)\n",
    "lr_schedule_classifier=torch.optim.lr_scheduler.StepLR(optimizer=optimizer_classifier,gamma=0.5,step_size=25)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"----pre-train----\")\n",
    "for ep in range(epoch):\n",
    "    train_loss=[]\n",
    "    print(\"----epoch: %2d---- \"%ep)\n",
    "    fe_model.train()\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch in tqdm(dloader_train(ep)):\n",
    "        assert(len(batch)==4)\n",
    "        \n",
    "        optimizer_fe.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        train_data=batch[0]\n",
    "        train_label=batch[1]\n",
    "        k_id=batch[2]\n",
    "        \n",
    "        if use_cuda:\n",
    "            train_data=train_data.cuda()\n",
    "            train_label=train_label.cuda()\n",
    "            k_id=k_id.cuda()\n",
    "        \n",
    "        batch_size,nTestBase,channels,width,high=train_data.size()\n",
    "        train_data=train_data.view(batch_size*nTestBase,channels,width,high)\n",
    "        train_data_embedding=fe_model(train_data)\n",
    "        pred_result=classifier(train_data_embedding.view(batch_size,nTestBase,-1),k_id)\n",
    "#         print(\"pred_result.size\",pred_result.size())\n",
    "        loss=criterion(pred_result.view(batch_size*nTestBase,-1),train_label.view(batch_size*nTestBase))\n",
    "        loss.backward()\n",
    "        optimizer_fe.step()\n",
    "        optimizer_classifier.step()\n",
    "        train_loss.append(float(loss))\n",
    "    lr_schedule_fe.step()\n",
    "    lr_schedule_classifier.step()\n",
    "    \n",
    "    avg_loss=np.mean(train_loss)\n",
    "    print(\"epoch %2d training end : avg_loss = %.4f\"%(ep,avg_loss))\n",
    "    with open(trace_file,'a') as f:\n",
    "        f.write('epoch:{:2d} training end：avg_loss:{:.4f}'.format(ep,avg_loss))\n",
    "        f.write('\\n')\n",
    "    if ep==epoch-1:\n",
    "        p1='results/pretrain_model/fe_%s.pth'%(str(ep))\n",
    "        p2='results/pretrain_model/classifier_%s.pth'%(str(ep))\n",
    "        m1=fe_model.save(path=p1)\n",
    "        m2=classifier.save(path=p2)\n",
    "        print(\"Epoch %2d model successfully saved!\"%(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training step2 : continue to train classifier and attention-based weight generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2 \n",
    "path_fe='results/pretrain_model/fe_30.pth'\n",
    "path_classifier='results/pretrain_model/classifier_30.pth'\n",
    "\n",
    "#load pretrain model\n",
    "fe_model=ConvNet()\n",
    "classifier=Classifier(weight_generator_type='attention_based')\n",
    "pre_train_classifier=torch.load(path_classifier)\n",
    "\n",
    "fe_model.load(path_fe)\n",
    "\n",
    "for pname , param in classifier.named_parameters():\n",
    "    if pname in pre_train_classifier:\n",
    "        param.data.copy_(pre_train_classifier[pname])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "epoch=60\n",
    "lr=0.1\n",
    "momentum=0.9\n",
    "weight_decay=5e-4\n",
    "\n",
    "dataset_train=MiniImageNet(phase='train')\n",
    "dataset_test=MiniImageNet(phase='val')\n",
    "\n",
    "dloader_train=FewShotDataloader(dataset=dataset_train,\n",
    "                               nKnovel=5,\n",
    "                               nKbase=-1,\n",
    "                               nExemplars=1,\n",
    "                               nTestNovel=5*3,\n",
    "                               nTestBase=5*3,\n",
    "                               batch_size=8,\n",
    "                               num_workers=1,\n",
    "                               epoch_size=8*1000)#8*1000\n",
    "dloader_test = FewShotDataloader(\n",
    "    dataset=dataset_test,\n",
    "    nKnovel=5,\n",
    "    nKbase=64,\n",
    "    nExemplars=1, # num training examples per novel category\n",
    "    nTestNovel=15*5, # num test examples for all the novel categories\n",
    "    nTestBase=15*5, # num test examples for all the base categories\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    epoch_size=2000, #2000 num of batches per epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_train_one_hot(labels_train,num_classes):\n",
    "    res=[]\n",
    "    batch_size,num=labels_train.size()\n",
    "    for i in range(batch_size):\n",
    "        min_value=torch.min(labels_train[i])\n",
    "        labels=labels_train[i]-min_value\n",
    "        one_hot=torch.zeros((num,num_classes))\n",
    "        for i in range(len(labels)):\n",
    "            one_hot[i][labels[i]]=1\n",
    "        res.append(one_hot)\n",
    "    return torch.cat(res).view(batch_size,num,num_classes)\n",
    "        \n",
    "def get_acc(pred,labels):\n",
    "    _,pred_inds=pred.max(dim=1)\n",
    "    pred_inds=pred_inds.view(-1)\n",
    "    labels=labels.view(-1)\n",
    "    acc=100*pred_inds.eq(labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('results/stage_2_model'):\n",
    "    os.makedirs('results/stage_2_model')\n",
    "\n",
    "trace_file=os.path.join('results','trace_file','train_stage_2_trace.txt')\n",
    "if os.path.isfile(trace_file):\n",
    "    os.remove(trace_file)\n",
    "    \n",
    "if use_cuda:\n",
    "    fe_model.cuda()\n",
    "    classifier.cuda()\n",
    "\n",
    "#optimizer\n",
    "# optimizer_fe=torch.optim.SGD(fe_model.parameters(),lr=lr,nesterov=True , momentum=momentum,weight_decay=weight_decay)\n",
    "optimizer_classifier=torch.optim.SGD(classifier.parameters(),lr=lr,nesterov=True , momentum=momentum,weight_decay=weight_decay)\n",
    "lr_schedule_classifier=torch.optim.lr_scheduler.StepLR(optimizer=optimizer_classifier,gamma=0.5,step_size=25)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"---- train-stage-2 ----\")\n",
    "best_acc_both=0.0\n",
    "best_acc_novel=0.0\n",
    "for ep in range(epoch):\n",
    "    train_loss=[]\n",
    "    acc_both=[]\n",
    "    acc_base=[]\n",
    "    acc_novel=[]\n",
    "    print(\"----epoch: %2d---- \"%ep)\n",
    "    fe_model.train()\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch in tqdm(dloader_train(ep)):\n",
    "        assert(len(batch)==6) #images_train, labels_train, images_test, labels_test, K, nKbase\n",
    "        \n",
    "#         optimizer_fe.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        train_data=batch[0]\n",
    "        train_label=batch[1]\n",
    "        test_data=batch[2]\n",
    "        test_label=batch[3]\n",
    "        k_id=batch[4]\n",
    "        nKbase=batch[5]\n",
    "        KbaseId=k_id[:,:nKbase[0]]\n",
    "        labels_train_one_hot=get_labels_train_one_hot(train_label,dloader_train.nKnovel)\n",
    "        \n",
    "        if use_cuda:\n",
    "            train_data=train_data.cuda()\n",
    "            train_label=train_label.cuda()\n",
    "            test_data=test_data.cuda()\n",
    "            test_label=test_label.cuda()\n",
    "            k_id=k_id.cuda()\n",
    "            nKbase=nKbase.cuda()\n",
    "            KbaseId=KbaseId.cuda()\n",
    "            labels_train_one_hot=labels_train_one_hot.cuda()\n",
    "        \n",
    "        batch_size,nExamples,channels,width,high=train_data.size()\n",
    "        nTest=test_data.size(1)\n",
    "        \n",
    "        train_data=train_data.view(batch_size*nExamples,channels,width,high)\n",
    "        test_data=test_data.view(batch_size*nTest,channels,width,high)\n",
    "        \n",
    "        train_data_embedding=fe_model(train_data)\n",
    "        test_data_embedding=fe_model(test_data)\n",
    "        \n",
    "        pred_result=classifier(features_test=test_data_embedding.view(batch_size,nTest,-1),Kbase_ids=KbaseId,\n",
    "                               features_train=train_data_embedding.view(batch_size,nExamples,-1),labels_train=labels_train_one_hot)\n",
    "#         print(\"pred_result.size\",pred_result.size())\n",
    "        pred_result = pred_result.view(batch_size*nTest,-1)\n",
    "        test_label = test_label.view(batch_size*nTest)\n",
    "    \n",
    "        loss=criterion(pred_result,test_label)\n",
    "        loss.backward()\n",
    "#         optimizer_fe.step()\n",
    "        optimizer_classifier.step()\n",
    "    \n",
    "        train_loss.append(float(loss))\n",
    "        \n",
    "        accuracy_both=get_acc(pred_result,test_label)\n",
    "        acc_both.append(float(accuracy_both))\n",
    "        \n",
    "        base_ids=torch.nonzero(test_label < nKbase[0]).view(-1)\n",
    "        novel_ids=torch.nonzero(test_label >= nKbase[0]).view(-1)\n",
    "        \n",
    "        pred_base = pred_result[base_ids,:]\n",
    "        pred_novel =pred_result[novel_ids,:]\n",
    "        \n",
    "        accuracy_base=get_acc(pred_base[:,:nKbase[0]],test_label[base_ids])\n",
    "        accuracy_novel=get_acc(pred_novel[:,nKbase[0]:],(test_label[novel_ids]-nKbase[0]))\n",
    "        \n",
    "        acc_base.append(float(accuracy_base))\n",
    "        acc_novel.append(float(accuracy_novel))\n",
    "        \n",
    "    \n",
    "    lr_schedule_classifier.step()\n",
    "    #------------------------------------------------------\n",
    "    #validation stage\n",
    "    print(\"----begin validation----\")\n",
    "    fe_model.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    val_loss=[]\n",
    "    val_acc_both=[]\n",
    "    val_acc_base=[]\n",
    "    val_acc_novel=[]\n",
    "    for batch in tqdm(dloader_test(ep)):\n",
    "        assert(len(batch)==6)\n",
    "        train_data=batch[0]\n",
    "        train_label=batch[1]\n",
    "        test_data=batch[2]\n",
    "        test_label=batch[3]\n",
    "        k_id=batch[4]\n",
    "        nKbase=batch[5]\n",
    "        KbaseId=k_id[:,:nKbase[0]]\n",
    "        labels_train_one_hot=get_labels_train_one_hot(train_label,dloader_test.nKnovel)\n",
    "        \n",
    "        if use_cuda:\n",
    "            train_data=train_data.cuda()\n",
    "            train_label=train_label.cuda()\n",
    "            test_data=test_data.cuda()\n",
    "            test_label=test_label.cuda()\n",
    "            k_id=k_id.cuda()\n",
    "            nKbase=nKbase.cuda()\n",
    "            KbaseId=KbaseId.cuda()\n",
    "            labels_train_one_hot=labels_train_one_hot.cuda()\n",
    "        \n",
    "        batch_size,nExamples,channels,width,high=train_data.size()\n",
    "        nTest=test_data.size(1)\n",
    "        \n",
    "        train_data=train_data.view(batch_size*nExamples,channels,width,high)\n",
    "        test_data=test_data.view(batch_size*nTest,channels,width,high)\n",
    "        \n",
    "        train_data_embedding=fe_model(train_data)\n",
    "        test_data_embedding=fe_model(test_data)\n",
    "        \n",
    "        pred_result=classifier(features_test=test_data_embedding.view(batch_size,nTest,-1),Kbase_ids=KbaseId,\n",
    "                               features_train=train_data_embedding.view(batch_size,nExamples,-1),labels_train=labels_train_one_hot)\n",
    "#         print(\"pred_result.size\",pred_result.size())\n",
    "        pred_result = pred_result.view(batch_size*nTest,-1)\n",
    "        test_label = test_label.view(batch_size*nTest)\n",
    "        \n",
    "        loss=criterion(pred_result,test_label)\n",
    "        val_loss.append(float(loss))\n",
    "        \n",
    "        accuracy_both=get_acc(pred_result,test_label)\n",
    "        val_acc_both.append(float(accuracy_both))\n",
    "        \n",
    "        base_ids=torch.nonzero(test_label < nKbase[0]).view(-1)\n",
    "        novel_ids=torch.nonzero(test_label >= nKbase[0]).view(-1)\n",
    "        \n",
    "        pred_base = pred_result[base_ids,:]\n",
    "        pred_novel =pred_result[novel_ids,:]\n",
    "        \n",
    "        accuracy_base=get_acc(pred_base[:,:nKbase[0]],test_label[base_ids])\n",
    "        accuracy_novel=get_acc(pred_novel[:,nKbase[0]:],(test_label[novel_ids]-nKbase[0]))\n",
    "        \n",
    "        val_acc_base.append(float(accuracy_base))\n",
    "        val_acc_novel.append(float(accuracy_novel))\n",
    "    avg_loss=np.mean(train_loss)\n",
    "    avg_acc_both=np.mean(acc_both)\n",
    "    avg_acc_base=np.mean(acc_base)\n",
    "    avg_acc_novel=np.mean(acc_novel)\n",
    "    \n",
    "    val_avg_loss=np.mean(val_loss)\n",
    "    val_avg_acc_both=np.mean(val_acc_both)\n",
    "    val_avg_acc_base=np.mean(val_acc_base)\n",
    "    val_avg_acc_novel=np.mean(val_acc_novel)\n",
    "    \n",
    "    print(\"epoch %2d training end : training ---- avg_loss = %.4f , avg_acc_both = %.2f , avg_acc_base = %.2f , avg_acc_novel = %.2f \"%(ep,avg_loss,avg_acc_both,avg_acc_base,avg_acc_novel))\n",
    "    print(\"epoch %2d training end : validation ---- avg_loss = %.4f , avg_acc_both = %.2f , avg_acc_base = %.2f , avg_acc_novel = %.2f \"%(ep,val_avg_loss,val_avg_acc_both,val_avg_acc_base,val_avg_acc_novel))\n",
    "    with open(trace_file,'a') as f:\n",
    "        f.write('epoch:{:2d}  training ---- avg_loss:{:.4f} , avg_acc_both:{:.2f} , avg_acc_base:{:.2f} , avg_acc_novel:{:.2f}'.format(ep,avg_loss,avg_acc_both,avg_acc_base,avg_acc_novel))\n",
    "        f.write('\\n')\n",
    "        f.write('epoch:{:2d}  validation ---- avg_loss:{:.4f} , avg_acc_both:{:.2f} , avg_acc_base:{:.2f} , avg_acc_novel:{:.2f}'.format(ep,val_avg_loss,val_avg_acc_both,val_avg_acc_base,val_avg_acc_novel))\n",
    "        f.write('\\n')\n",
    "    if best_acc_both<val_avg_acc_both:\n",
    "        print(\"produce best both_acc model，saving------\")\n",
    "        \n",
    "        p1='results/stage_2_model/fe_best_both.pth'\n",
    "        p2='results/stage_2_model/classifier_best_both.pth'\n",
    "        m1=fe_model.save(path=p1)\n",
    "        m2=classifier.save(path=p2)\n",
    "        best_acc_both=avg_acc_both\n",
    "        print(\"successfully saving current best both_acc model----\")\n",
    "    if best_acc_novel<val_avg_acc_novel:\n",
    "        print(\"produce best novel_acc model，saving------\")\n",
    "        \n",
    "        p1='results/stage_2_model/fe_best_novel.pth'\n",
    "        p2='results/stage_2_model/classifier_best_novel.pth'\n",
    "        m1=fe_model.save(path=p1)\n",
    "        m2=classifier.save(path=p2)\n",
    "        best_acc_novel=avg_acc_novel\n",
    "        print(\"succewssfully saving current best novel_acc model----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test stage\n",
    "path_fe='results/stage_2_model/fe_best_novel.pth'\n",
    "path_classifier='results/stage_2_model/classifier_best_novel.pth'\n",
    "\n",
    "#load model\n",
    "fe_model=ConvNet()\n",
    "classifier=Classifier(weight_generator_type='attention_based')\n",
    "\n",
    "fe_model.load(path_fe)\n",
    "classifier.load(path_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "\n",
    "dataset_test=MiniImageNet(phase='test')\n",
    "\n",
    "\n",
    "dloader_test = FewShotDataloader(\n",
    "    dataset=dataset_test,\n",
    "    nKnovel=5,\n",
    "    nKbase=64,\n",
    "    nExemplars=1, # num training examples per novel category\n",
    "    nTestNovel=15*5, # num test examples for all the novel categories\n",
    "    nTestBase=15*5, # num test examples for all the base categories\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    epoch_size=600, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_file=os.path.join('results','trace_file','test_trace.txt')\n",
    "if os.path.isfile(trace_file):\n",
    "    os.remove(trace_file)\n",
    "    \n",
    "if use_cuda:\n",
    "    fe_model.cuda()\n",
    "    classifier.cuda()\n",
    "\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"---- test-stage ----\")\n",
    "fe_model.eval()\n",
    "classifier.eval()\n",
    "test_loss=[]\n",
    "test_acc_both=[]\n",
    "test_acc_base=[]\n",
    "test_acc_novel=[]\n",
    "for batch in tqdm(dloader_test()):\n",
    "    assert(len(batch)==6)\n",
    "    train_data=batch[0]\n",
    "    train_label=batch[1]\n",
    "    test_data=batch[2]\n",
    "    test_label=batch[3]\n",
    "    k_id=batch[4]\n",
    "    nKbase=batch[5]\n",
    "    KbaseId=k_id[:,:nKbase[0]]\n",
    "    labels_train_one_hot=get_labels_train_one_hot(train_label,dloader_test.nKnovel)\n",
    "\n",
    "    if use_cuda:\n",
    "        train_data=train_data.cuda()\n",
    "        train_label=train_label.cuda()\n",
    "        test_data=test_data.cuda()\n",
    "        test_label=test_label.cuda()\n",
    "        k_id=k_id.cuda()\n",
    "        nKbase=nKbase.cuda()\n",
    "        KbaseId=KbaseId.cuda()\n",
    "        labels_train_one_hot=labels_train_one_hot.cuda()\n",
    "\n",
    "    batch_size,nExamples,channels,width,high=train_data.size()\n",
    "    nTest=test_data.size(1)\n",
    "\n",
    "    train_data=train_data.view(batch_size*nExamples,channels,width,high)\n",
    "    test_data=test_data.view(batch_size*nTest,channels,width,high)\n",
    "\n",
    "    train_data_embedding=fe_model(train_data)\n",
    "    test_data_embedding=fe_model(test_data)\n",
    "\n",
    "    pred_result=classifier(features_test=test_data_embedding.view(batch_size,nTest,-1),Kbase_ids=KbaseId,\n",
    "                           features_train=train_data_embedding.view(batch_size,nExamples,-1),labels_train=labels_train_one_hot)\n",
    "#         print(\"pred_result.size\",pred_result.size())\n",
    "    pred_result = pred_result.view(batch_size*nTest,-1)\n",
    "    test_label = test_label.view(batch_size*nTest)\n",
    "\n",
    "    loss=criterion(pred_result,test_label)\n",
    "    test_loss.append(float(loss))\n",
    "\n",
    "    accuracy_both=get_acc(pred_result,test_label)\n",
    "    test_acc_both.append(float(accuracy_both))\n",
    "\n",
    "    base_ids=torch.nonzero(test_label < nKbase[0]).view(-1)\n",
    "    novel_ids=torch.nonzero(test_label >= nKbase[0]).view(-1)\n",
    "\n",
    "    pred_base = pred_result[base_ids,:]\n",
    "    pred_novel =pred_result[novel_ids,:]\n",
    "\n",
    "    accuracy_base=get_acc(pred_base[:,:nKbase[0]],test_label[base_ids])\n",
    "    accuracy_novel=get_acc(pred_novel[:,nKbase[0]:],(test_label[novel_ids]-nKbase[0]))\n",
    "\n",
    "    test_acc_base.append(float(accuracy_base))\n",
    "    test_acc_novel.append(float(accuracy_novel))\n",
    "\n",
    "test_avg_loss=np.mean(test_loss)\n",
    "test_avg_acc_both=np.mean(test_acc_both)\n",
    "test_avg_acc_base=np.mean(test_acc_base)\n",
    "test_avg_acc_novel=np.mean(test_acc_novel)\n",
    "\n",
    "print(\"%2d batch test end :  avg_loss = %.4f , avg_acc_both = %.2f , avg_acc_base = %.2f , avg_acc_novel = %.2f \"%(dloader_test.epoch_size,test_avg_loss,test_avg_acc_both,test_avg_acc_base,test_avg_acc_novel))\n",
    "with open(trace_file,'a') as f:\n",
    "    f.write('batch_size:{:2d}  test ---- avg_loss:{:.4f} , avg_acc_both:{:.2f} , avg_acc_base:{:.2f} , avg_acc_novel:{:.2f}'.format(dloader_test.epoch_size,test_avg_loss,test_avg_acc_both,test_avg_acc_base,test_avg_acc_novel))\n",
    "    f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
